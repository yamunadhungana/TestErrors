---
title: "Modern Applied Statistics exercises from ISLR"
author: "Yamuna Dhungana"
output:
   pdf_document:
     latex_engine: xelatex
  
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F,warning=F,message=F)
```


```{r}
library(ISLR)
library(ggplot2)
library(GGally)

```

This analysis employs the Weekly dataset, which covers the percentage returns for the S&P 500 stock index spanning from 1990 to 2010. The dataset is organized as a data frame with 1089 observations related to nine variables: Year, Lag1, Lag2, Lag3, Lag4, Lag5, Volume, Today, and Direction. The following numerical and graphical summaries are presented to identify any discernible patterns.


```{r,echo=FALSE,warning=FALSE}

data("Weekly")
# head(Weekly)
# names(Weekly)


# for the numerical summary
summary(Weekly)

# corelation of the data
cor(Weekly[,-9])

# from the correlation we found out that Volume and year are highly correlated


# pairs(Weekly)
ggpairs(Weekly)

plot(Weekly$Volume ~ Weekly$Year, 
     main = "Volume vs Year",
     xlab = "Year",
     ylab = "Volume"       )
plot(Weekly$Volume, 
     data = Weekly, 
     ylab = "Volume",
     main = "Scatterplot for Volume")
qplot(Weekly$Volume, 
      data = Weekly,
      xlab = "Volume",
      main = "qplot for Volume")


```

The correlation analysis of the 'weekly' dataset reveals a robust association between volume and year. In contrast, other variables do not exhibit a similarly pronounced correlation. Additionally, a visualization of the year and volume variables suggests a gradual exponential rise from 1995 to 2004. Subsequently, for the subsequent years, there appears to be a consistent increase in volume, with a slight decline noted in 2010.
Conducting logistic regression on the entire dataset involves employing Direction as the response variable and utilizing the five lag variables along with Volume as predictors. Additionally, an examination will be conducted to assess the statistical significance of the regression results.

```{r,echo=FALSE,warning=FALSE}
fit_log <- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, data = Weekly,
               family = binomial)
summary(fit_log)

```

 According to the model summary, it is evident that only lag2 exhibits statistical significance, with a p-value of 0.0296, meeting the criteria of P < 0.05. The estimated coefficient for lag2 is 0.05844, signifying that, holding the other predictors constant, there is an anticipated mean increase in log odds as the stock market rises by a unit increase in lag2. Beyond this, the deviance residual of the model indicates a positive skewness in the data.
I am calculating the confusion matrix and the overall fraction of correct predictions. Additionally, I aim to identify the specific types of errors made by the logistic regression model.


```{r,echo=FALSE,warning=FALSE}

# # library(caret)
# # pred_data <- predict()
# 
# # pred_data <- predict(fit_log, type = "response")
# # pred.glm <- rep("Down", length(pred_data))
# # pred.glm[pred_data > 0.5] <- "Up"
# # table(pred.glm, Direction)

do.confusion=function(Th.hold,model,data){
  preds=rep("Down",dim(data)[1])
  vals=predict(model,newdata=data,type="response")
  for(i in 1:dim(data)[1]){
    if(vals[i]>=Th.hold){
      preds[i]="Up"
    }
  }
  ## Confusion matrix
  print("Confusion Matrix:")
  con=table(preds,data$Direction)
  print(con)
  
}
do.confusion(0.5,fit_log,Weekly)

```
   
  The confusion matrix delineates correct and erroneous predictions made by the model. It comprises four distinct factors: True Positive, True Negative, False Positive, and False Negative. True Positive and True Negative signify correct predictions, while False Positive and False Negative denote incorrect ones. In our matrix, the model accurately predicted the direction as up and down in 557 and 54 instances, respectively. The value 48 represents false positives, where the model predicted an upward direction, but the actual direction was down. The value 430 indicates false negatives, signifying instances where the model predicted a downward direction, but the actual direction was up.

Furthermore, we can calculate the test error from the matrix using the formula `(54 + 48) / 1089`, yielding a percentage of correct predictions at 56.10%. Additionally, if the model predicts an upward direction, it will be correct 92.06% of the time (`557 / (48 + 557)`), while for a downward direction, the correctness rate is 11.15% (`54 / (54 + 430)`).

Now, I am fitting the logistic regression model using training data spanning from 1990 to 2008, where Lag2 serves as the sole predictor. Following this, I will compute the confusion matrix and determine the overall fraction of correct predictions for the held-out data, specifically the data from 2009 and 2010.

```{r,echo=FALSE,warning=FALSE}

new_data <- c(which(Weekly$Year==2009), which(Weekly$Year==2010))
test_data <- Weekly[new_data,]
train <- Weekly[-new_data,]
fit_log2 <- glm(Direction~ Lag2, data = train, family= binomial)
summary(fit_log2)



 summary(test_data$Direction)

# For confusion matrix

do.confusion(0.5,fit_log2,Weekly)

```
  
 In our model, there are 43 instances of the total data being down and 61 instances being up. Within the confusion matrix, our accurate predictions for the upward and downward directions are 580 and 32, respectively. The value 25 represents false positives, indicating instances where the model predicted an upward direction, but the actual direction was down. The value 452 represents false negatives, signifying cases where the model predicted a downward direction, but the actual direction was up.

Furthermore, the test error can be computed from the matrix using the formula `(32 + 25) / 1089`, resulting in a percentage of correct predictions at 56.19%. Specifically, when the model predicts an upward direction, it is correct 95.86% of the time (`580 / (25 + 580)`), while for a downward direction, the correctness rate is 5.22% (`32 / (32 + 580)`).



In an attempt to develop a model for predicting whether a given car has high or low gas mileage using the Auto dataset, I am creating a binary variable named 'mpg01.' This variable takes the value 1 if the 'mpg' variable contains a value above its median and 0 if 'mpg' contains a value below its median. The median can be computed using the median() function.

```{r,echo=FALSE,warning=FALSE}
library(ISLR)
data(Auto)
mpg01 <- rep(NA,dim(Auto)[1])
med_ian <- median(Auto$mpg)
mpg01 = ifelse(Auto$mpg<med_ian,0,1)
my_data = as.data.frame(cbind(Auto, mpg01))
head(my_data)

```


Examining the data graphically to explore the relationship between 'mpg01' and the remaining features. Identifying which of the other features appear to be most relevant for predicting 'mpg01' by plotting scatterplots and boxplots.


```{r,echo=FALSE,warning=FALSE}
# for selecting only the required variables 
 
v <- c(2,3,4,5,6,7,8)
layout(matrix(1:4,nrow = 2))
for (i in v){
   boxplot(my_data[,i] ~ my_data$mpg01,
          col = rainbow(7), 
          xlab="mpg01", 
          ylab= names(my_data)[i], 
          main= paste0("Box plot for the mpg01 and ", names(my_data)[i])
          )
}

 
newdf <- my_data[,c(2,3,4,5,6,7,8,10)] # excluding mpg and names from my_data
plot(newdf,pch=16,cex=0.9,col=2)

# correlation of data
cor(newdf)

# plot for horsepower and displacement
plot(horsepower ~ displacement,
     Auto,
     pch=16,
     cex=0.8,
     col=2, 
     main = "Horsepower vs Displacement")

library(ggplot2)
library(GGally)
#pairs(newdf) #pairwise correlation
ggpairs(newdf,cardinality_threshold = 15)#ggpairs
# str(newdf)

```

 The box plot clearly indicates a discernible distinction in the distribution between two groups for the variables cylinders, horsepower, displacement, weight, origin, and year. Notably, a majority of the automobiles originated in Japan. Cars from the United States are predominantly concentrated at lower mpg, whereas European and Japanese cars exhibit a more even distribution. Additionally, older cars generally tend to have lower mpg, while modern cars tend to have higher mpg.

Moreover, the correlation plot reveals significant correlations among the physical attributes of the car. Notably, there appears to be a high correlation between displacement and horsepower, suggesting an exponential relationship between the two.
Splitted the data in the ration of 70% and 30% .

```{r,echo=FALSE,warning=FALSE}
library(caTools)
sample.split(my_data,SplitRatio = 0.70)-> mysplit
subset(my_data,mysplit==T)->train
subset(my_data,mysplit==F)->test
#dim(train)

```

Conducting logistic regression on the training data to predict 'mpg01' using the variables that exhibited the strongest associations with 'mpg01.'

```{r,echo=FALSE,warning=FALSE}
fit.log3 <- glm(mpg01 ~ cylinders + weight + displacement + horsepower, data = train, family = binomial)
summary(fit.log3)

# test error

test.err=function(cutoff,model,test){
  preds=rep(0,dim(test)[1])
  probs=predict(model,newdata=test, type="response")
  for(i in 1:length(probs)){
    if(probs[i]>=cutoff){
      preds[i]=1
    }
  }
  cm=table(preds, test$mpg01)
  message("Confusion Matrix:");print(cm)
  ac=((cm[1,1]+cm[2,2])/sum(cm))*100
  message("Overall test accuracy (percentage) : ", round(ac,2))
  paste0("Test error (percantage): ",round((100-ac),2))
  
}

test.err(0.5,fit.log3, test)


```

Based on the findings from question b, where cylinders, weight, displacement, and horsepower were identified as variables most associated with 'mpg01,' logistic regression was performed using these variables. In the computed model, it was determined that weight and horsepower are statistically significant predictors. Additionally, the model exhibited a negative skewness in the data.

For evaluating test accuracy, a confusion matrix was generated, revealing that 88.14% of the data was correctly predicted, while 11.86% was predicted incorrectly. Consequently, the test error for the model stands at 11.86%. 



```{r,echo=FALSE,warning=FALSE}

# https://www.jigsawacademy.com/sensitivity-vs-specificity-in-logistic-regression/
# https://medium.com/analytics-vidhya/a-guide-to-machine-learning-in-r-for-beginners-part-5-4c00f2366b90
# https://bookdown.org/yihui/rmarkdown-cookbook/kable.html
# https://www.theanalysisfactor.com/sensitivity-and-specificity/
# https://www.datamentor.io/r-programming/if-else-statement/
# https://stackoverflow.com/questions/46028360/confusionmatrix-for-logistic-regression-in-r
# https://stats.stackexchange.com/questions/65244/how-to-determine-the-accuracy-of-logistic-regression-in-r


```
    
